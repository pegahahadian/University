{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-Agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNn/67yzl4jKMnMza5de9SJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pegahahadian/university/blob/main/Q_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDmwR4KCbSMe",
        "outputId": "5b64c1bd-9794-4966-df00-7466f50e4630"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibLpwknnbcTr"
      },
      "source": [
        "Reinforcement learning is learning how to map situations to actions so as to maximize a numerical reward signal. Gym is a toolkit for developing and comparing reinforcement learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht9xb-Csbc2I"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDcDZmv5b2VZ"
      },
      "source": [
        "Play one game of blackjack with random actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmx9huHcb24n"
      },
      "source": [
        "env = gym.make(\"Blackjack-v0\")\n",
        "observation = env.reset()\n",
        "memory = []\n",
        "for _ in range(10):\n",
        "  action = env.action_space.sample() \n",
        "  observation, reward, done, info = env.step(action)\n",
        "  memory.append((observation,action,reward,done))\n",
        "  if done:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce2HiUKmb7ke"
      },
      "source": [
        "Calling env.step gives us an observation, reward, a boolean indicating whether the episode has finished"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsxvKbOIb8GF",
        "outputId": "122a8228-cb5e-418f-d373-648d747d11ce"
      },
      "source": [
        "memory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((14, 2, False), 1, 0.0, False), ((14, 2, False), 0, 1.0, True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnWVnHnocAC1"
      },
      "source": [
        "**States**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXAHL-8qcAxm"
      },
      "source": [
        " The observation is a 3-tuple of: \n",
        "the players current sum,\n",
        "the dealer's one showing card (1-10 where 1 is ace),\n",
        "and whether or not the player holds a usable ace (0 or 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yv9Cs7dcDOe"
      },
      "source": [
        "**Actions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8iiHrAQcLc6",
        "outputId": "1f2d1330-fd55-42e3-8ca3-f89c6fa4afe0"
      },
      "source": [
        "env.action_space\n",
        "# Stay = 0\n",
        "# Hit = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP0Rgrd8cOJD"
      },
      "source": [
        "**Rewards**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAXIbfvScQES"
      },
      "source": [
        "# Win = 1\n",
        "# Loss = -1\n",
        "\n",
        "def compute_avg_reward(memory):\n",
        "  rewards = [r[2] for r in memory]\n",
        "  return sum(rewards)/len(memory)\n",
        "\n",
        "#compute_avg_reward(memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tfeg_tccTHP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqWv2ZlWcTPZ"
      },
      "source": [
        "Now lets play 100 games of random blackjack. We'll keep track of our score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwFo4BQacUX8"
      },
      "source": [
        "env = gym.make(\"Blackjack-v0\")\n",
        "observation = env.reset()\n",
        "memory = []\n",
        "episodes = 100\n",
        "for e in range(episodes):\n",
        "  for _ in range(10):\n",
        "    action = env.action_space.sample() \n",
        "    observation, reward, done, info = env.step(action)\n",
        "    memory.append((observation,action,reward,done))\n",
        "    if done:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG3GaoiKcYxb",
        "outputId": "b6db4dde-67e5-4b92-c5ac-1d77f44c935e"
      },
      "source": [
        "rewards = [r[2] for r in memory]\n",
        "sum(rewards)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-100.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeRIMHS6cbzq"
      },
      "source": [
        "Let's try building a simple agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEL91CjlccUM"
      },
      "source": [
        "class RuleBasedAgent():\n",
        "  \n",
        "  def __init__(self,hit_probability):\n",
        "    self.hit_probability = hit_probability\n",
        "  \n",
        "  def act(self,state):\n",
        "    if state[0]>=17:\n",
        "      return 0\n",
        "    else:\n",
        "      return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZnr85IGcgYq"
      },
      "source": [
        "env = gym.make(\"Blackjack-v0\")\n",
        "\n",
        "memory = []\n",
        "agent = RuleBasedAgent(.8)\n",
        "episodes = 100\n",
        "for e in range(episodes):\n",
        "  state = env.reset()\n",
        "  for _ in range(10):\n",
        "    action = agent.act(observation) \n",
        "    state, reward, done, info = env.step(action)\n",
        "    memory.append((observation,action,reward,done))\n",
        "    if done:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxnaMtiKci9w",
        "outputId": "0d861abc-2774-40ae-943a-e013b3ca6b47"
      },
      "source": [
        "compute_avg_reward(memory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeK2RKuMcl1_"
      },
      "source": [
        "**A reinforcement learning policy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjzfPIpZcpNq"
      },
      "source": [
        "One of the things that sets reinforcement learning apart from machine learning is that the agent explores its environment to discover the optimal policy. A policy specifies what action to take at every state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws9DyvuncqsT"
      },
      "source": [
        "def epsilon_greedy(agent):\n",
        "  if random.random()>.3:\n",
        "    return env.action_space.sample()\n",
        "  else:\n",
        "    return agent.act()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMbg9-sjcxYx"
      },
      "source": [
        "The task of the agent is estimate the value of taking an action in a given state. This is referred to as the Q-value. The class of algorithms that estimate Q-values are called Q-learning. Q can be estimated a variety of ways including Monte Carlo methods and neural networks. We will show an example with a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBtdlh7tcyQA"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "class NeuralQAgent():\n",
        "  \n",
        "  def __init__(self,epsilon):\n",
        "    self.epsilon = epsilon\n",
        "    self.memory = []\n",
        "    self.init_model()\n",
        "    \n",
        "  def init_model(self):\n",
        "    inputs = Input(shape=(3,))\n",
        "    x = Dense(5,activation='relu')(inputs)\n",
        "    x1 = Dense(5,activation='relu')(x)\n",
        "    output = Dense(2,activation='softmax')(x1)\n",
        "\n",
        "    model = Model(inputs=inputs,outputs=output)\n",
        "    model.compile(optimizer='adam',loss='mean_squared_error')\n",
        "    self.model = model\n",
        "    \n",
        "  def act(self,state):\n",
        "    state = np.array(state,ndmin=2)\n",
        "\n",
        "    if random.random()>self.epsilon:\n",
        "      return env.action_space.sample()\n",
        "    else:\n",
        "      self.update_Q()\n",
        "      action = np.argmax(self.model.predict(state))\n",
        "      return action\n",
        "\n",
        "    \n",
        "  def update_Q(self):\n",
        "    if len(self.memory)<32:\n",
        "      pass\n",
        "    else:\n",
        "      batch = random.sample(self.memory,32)\n",
        "      states = np.array([np.array(s[0]) for s in batch])\n",
        "      actions = np.array([a[1] for a in batch])\n",
        "      rewards = np.array([a[2] for a in batch])\n",
        "      targets = self.model.predict(states)\n",
        "      for i in range(len(targets)):\n",
        "        targets[i][actions[i]] = rewards[i]\n",
        "      self.model.fit(states,targets,verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCc7Yz1yc9Tn"
      },
      "source": [
        "env = gym.make(\"Blackjack-v0\")\n",
        "\n",
        "\n",
        "agent = NeuralQAgent(.1)\n",
        "\n",
        "episodes = 100000\n",
        "for e in range(episodes):\n",
        "  state = env.reset()\n",
        "  while True:\n",
        "    action = agent.act(state) \n",
        "    state, reward, done, info = env.step(action)\n",
        "    agent.memory.append((state,action,reward,done))\n",
        "    if done:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH1XUKhukS7E",
        "outputId": "46e6f86c-4b9e-4f54-fe9e-9d293dd360e6"
      },
      "source": [
        "compute_avg_reward(agent.memory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.27001510857881644"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}